{
  "localModels": {
    "version": 1,
    "lastUpdated": 1706745600000,
    "families": [
      {
        "id": "llama3",
        "name": "Llama 3.2",
        "description": "Meta's latest open-source LLM, optimized for mobile and edge devices",
        "developer": "Meta",
        "license": "Llama 3.2 Community License",
        "websiteUrl": "https://ai.meta.com/llama/"
      },
      {
        "id": "qwen2",
        "name": "Qwen 2.5",
        "description": "Alibaba's powerful multilingual language model",
        "developer": "Alibaba Cloud",
        "license": "Apache 2.0",
        "websiteUrl": "https://github.com/QwenLM/Qwen2.5"
      },
      {
        "id": "smollm",
        "name": "SmolLM2",
        "description": "Hugging Face's tiny but capable language models",
        "developer": "Hugging Face",
        "license": "Apache 2.0",
        "websiteUrl": "https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B"
      }
    ],
    "models": [
      {
        "id": "llama-3.2-1b-q4",
        "name": "Llama 3.2 1B",
        "description": "Compact model ideal for on-device chat and simple tasks. Fast inference with minimal memory usage.",
        "size": 750000000,
        "downloadUrl": "https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_K_M.gguf",
        "fileName": "llama-3.2-1b-q4.gguf",
        "performance": {
          "tokensPerSecond": 25,
          "memoryRequired": 1500000000,
          "cpuIntensive": false,
          "gpuAccelerated": true,
          "rating": "FAST"
        },
        "useCases": ["CHAT", "GENERAL", "QUESTION_ANSWERING"],
        "quantization": "Q4_K_M",
        "parameters": "1B",
        "contextLength": 131072,
        "familyId": "llama3",
        "isRecommended": true,
        "isEnabled": true
      },
      {
        "id": "llama-3.2-3b-q4",
        "name": "Llama 3.2 3B",
        "description": "Balanced model with improved reasoning. Great for most on-device tasks.",
        "size": 2000000000,
        "downloadUrl": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf",
        "fileName": "llama-3.2-3b-q4.gguf",
        "performance": {
          "tokensPerSecond": 15,
          "memoryRequired": 3500000000,
          "cpuIntensive": false,
          "gpuAccelerated": true,
          "rating": "BALANCED"
        },
        "useCases": ["CHAT", "CODING", "CREATIVE", "GENERAL"],
        "quantization": "Q4_K_M",
        "parameters": "3B",
        "contextLength": 131072,
        "familyId": "llama3",
        "isRecommended": true,
        "isEnabled": true
      },
      {
        "id": "qwen2.5-0.5b-q4",
        "name": "Qwen 2.5 0.5B",
        "description": "Ultra-lightweight model for basic tasks. Extremely fast inference.",
        "size": 400000000,
        "downloadUrl": "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/qwen2.5-0.5b-instruct-q4_k_m.gguf",
        "fileName": "qwen2.5-0.5b-q4.gguf",
        "performance": {
          "tokensPerSecond": 40,
          "memoryRequired": 800000000,
          "cpuIntensive": false,
          "gpuAccelerated": true,
          "rating": "FAST"
        },
        "useCases": ["CHAT", "GENERAL"],
        "quantization": "Q4_K_M",
        "parameters": "0.5B",
        "contextLength": 32768,
        "familyId": "qwen2",
        "isRecommended": false,
        "isEnabled": true
      },
      {
        "id": "qwen2.5-1.5b-q4",
        "name": "Qwen 2.5 1.5B",
        "description": "Small but capable model with strong multilingual support.",
        "size": 1100000000,
        "downloadUrl": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q4_k_m.gguf",
        "fileName": "qwen2.5-1.5b-q4.gguf",
        "performance": {
          "tokensPerSecond": 22,
          "memoryRequired": 2000000000,
          "cpuIntensive": false,
          "gpuAccelerated": true,
          "rating": "FAST"
        },
        "useCases": ["CHAT", "TRANSLATION", "GENERAL"],
        "quantization": "Q4_K_M",
        "parameters": "1.5B",
        "contextLength": 32768,
        "familyId": "qwen2",
        "isRecommended": false,
        "isEnabled": true
      },
      {
        "id": "smollm2-135m-q8",
        "name": "SmolLM2 135M",
        "description": "Tiny model for ultra-fast responses. Perfect for simple queries.",
        "size": 150000000,
        "downloadUrl": "https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct-GGUF/resolve/main/smollm2-135m-instruct-q8_0.gguf",
        "fileName": "smollm2-135m-q8.gguf",
        "performance": {
          "tokensPerSecond": 60,
          "memoryRequired": 400000000,
          "cpuIntensive": false,
          "gpuAccelerated": false,
          "rating": "FAST"
        },
        "useCases": ["CHAT", "GENERAL"],
        "quantization": "Q8_0",
        "parameters": "135M",
        "contextLength": 8192,
        "familyId": "smollm",
        "isRecommended": false,
        "isEnabled": true
      },
      {
        "id": "smollm2-1.7b-q4",
        "name": "SmolLM2 1.7B",
        "description": "The largest SmolLM with impressive capabilities for its size.",
        "size": 1000000000,
        "downloadUrl": "https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct-GGUF/resolve/main/smollm2-1.7b-instruct-q4_k_m.gguf",
        "fileName": "smollm2-1.7b-q4.gguf",
        "performance": {
          "tokensPerSecond": 20,
          "memoryRequired": 2500000000,
          "cpuIntensive": false,
          "gpuAccelerated": true,
          "rating": "FAST"
        },
        "useCases": ["CHAT", "CREATIVE", "GENERAL"],
        "quantization": "Q4_K_M",
        "parameters": "1.7B",
        "contextLength": 8192,
        "familyId": "smollm",
        "isRecommended": true,
        "isEnabled": true
      }
    ]
  }
}
